{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1472241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46a74116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from the drive and saving it to Categories\n",
    "DATADIR =\"C:/Users/AKSHAY LATHWAL/Desktop/Capstone/Final Model/2,3 vs 4\"\n",
    "CATEGORIES = ['2,3','4']\n",
    "\n",
    "# creating the training dataset using the images of all 5 categories\n",
    "\n",
    "training_data = []\n",
    "IMG_SIZE = 100\n",
    "def create_training_data():  # function to create an array of images\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR,category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path,img))\n",
    "            try:\n",
    "                new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE)) # resizing the image\n",
    "            except:\n",
    "                continue\n",
    "            training_data.append([new_array,class_num])\n",
    "\n",
    "create_training_data()\n",
    "\n",
    "# empty lists to append images and labels\n",
    "X = []\n",
    "y = []\n",
    "# appending images to list X and labels to list Y\n",
    "for features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b955ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into train and test datasets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)\n",
    "\n",
    "# normalising the data\n",
    "X_train = X/255\n",
    "y_train = y\n",
    "#X_test = X_test/255\n",
    "\n",
    "# converting the train and test data into arrays\n",
    "X_train = np.array(X_train)\n",
    "#X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d31fcf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "68/68 [==============================] - 11s 156ms/step - loss: 0.6946 - accuracy: 0.5114\n",
      "Epoch 2/20\n",
      "68/68 [==============================] - 11s 156ms/step - loss: 0.6934 - accuracy: 0.5030\n",
      "Epoch 3/20\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.6927 - accuracy: 0.5049\n",
      "Epoch 4/20\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.6868 - accuracy: 0.5487\n",
      "Epoch 5/20\n",
      "68/68 [==============================] - 10s 153ms/step - loss: 0.6824 - accuracy: 0.5515\n",
      "Epoch 6/20\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.6672 - accuracy: 0.6033\n",
      "Epoch 7/20\n",
      "68/68 [==============================] - 11s 156ms/step - loss: 0.6338 - accuracy: 0.6378\n",
      "Epoch 8/20\n",
      "68/68 [==============================] - 10s 150ms/step - loss: 0.5917 - accuracy: 0.6723\n",
      "Epoch 9/20\n",
      "68/68 [==============================] - 10s 154ms/step - loss: 0.5773 - accuracy: 0.6793\n",
      "Epoch 10/20\n",
      "68/68 [==============================] - 11s 155ms/step - loss: 0.5429 - accuracy: 0.7161\n",
      "Epoch 11/20\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.4803 - accuracy: 0.7594\n",
      "Epoch 12/20\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.4302 - accuracy: 0.7804\n",
      "Epoch 13/20\n",
      "68/68 [==============================] - 11s 155ms/step - loss: 0.3668 - accuracy: 0.8322\n",
      "Epoch 14/20\n",
      "68/68 [==============================] - 11s 157ms/step - loss: 0.4512 - accuracy: 0.7809\n",
      "Epoch 15/20\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.4627 - accuracy: 0.7781\n",
      "Epoch 16/20\n",
      "68/68 [==============================] - 11s 167ms/step - loss: 0.3235 - accuracy: 0.8597\n",
      "Epoch 17/20\n",
      "68/68 [==============================] - 11s 168ms/step - loss: 0.2625 - accuracy: 0.8928\n",
      "Epoch 18/20\n",
      "68/68 [==============================] - 11s 157ms/step - loss: 0.1928 - accuracy: 0.9287\n",
      "Epoch 19/20\n",
      "68/68 [==============================] - 11s 158ms/step - loss: 0.3108 - accuracy: 0.8783\n",
      "Epoch 20/20\n",
      "68/68 [==============================] - 10s 153ms/step - loss: 0.1757 - accuracy: 0.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25e4bb24c70>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = models.Sequential([\n",
    "    \n",
    "    layers.Conv2D(filters=32, kernel_size=(2,2), activation='relu', input_shape=(IMG_SIZE,IMG_SIZE, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=64, kernel_size=(2,2), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=128, kernel_size=(2,2), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=128, kernel_size=(2,2), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(filters=256, kernel_size=(2,2), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    \n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "#Compiling the model\n",
    "model_2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#fitting model on training data\n",
    "model_2.fit(np.array(X_train), np.array(y_train), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96821264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Laoding Val_Test data\n",
    "DATADIR =\"C:/Users/AKSHAY LATHWAL/Desktop/Capstone/Final Model/Val_Test(2,3 vs 4)\"\n",
    "CATEGORIES = ['2,3','4']\n",
    "\n",
    "# creating the val_test dataset using the images of all 5 categories\n",
    "validation_data = []\n",
    "IMG_SIZE = 100\n",
    "def create_validation_data():  # function to create an array of images\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR,category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path,img))\n",
    "            try:\n",
    "                new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE)) # resizing the image\n",
    "            except:\n",
    "                continue\n",
    "            validation_data.append([new_array,class_num])\n",
    "\n",
    "create_validation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73a9b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists to append images and labels\n",
    "X = []\n",
    "y = []\n",
    "# appending images to list X and labels to list Y\n",
    "for features,label in validation_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X)\n",
    "\n",
    "# normalising the data\n",
    "X_test = X/255\n",
    "y_test = y\n",
    "\n",
    "# converting the train and test data into arrays\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1366a6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 37ms/step\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87       152\n",
      "           1       0.11      0.08      0.09        25\n",
      "\n",
      "    accuracy                           0.77       177\n",
      "   macro avg       0.48      0.48      0.48       177\n",
      "weighted avg       0.75      0.77      0.76       177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predictions on validation data\n",
    "y_pred2 = model_2.predict(X_test)\n",
    "y_pred2_classes = [np.argmax(element) for element in y_pred2]\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(y_test, y_pred2_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e500283",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.save('model_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246ab70a",
   "metadata": {},
   "source": [
    "# Experiment 2 : 100 image size with epoch = 40 & tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfeef610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from the drive and saving it to Categories\n",
    "DATADIR =\"C:/Users/AKSHAY LATHWAL/Desktop/Capstone/Final Model/2,3 vs 4\"\n",
    "CATEGORIES = ['2,3','4']\n",
    "\n",
    "# creating the training dataset using the images of all 5 categories\n",
    "\n",
    "training_data = []\n",
    "IMG_SIZE = 100\n",
    "def create_training_data():  # function to create an array of images\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR,category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path,img))\n",
    "            try:\n",
    "                new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE)) # resizing the image\n",
    "            except:\n",
    "                continue\n",
    "            training_data.append([new_array,class_num])\n",
    "\n",
    "create_training_data()\n",
    "\n",
    "# empty lists to append images and labels\n",
    "X = []\n",
    "y = []\n",
    "# appending images to list X and labels to list Y\n",
    "for features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f011e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into train and test datasets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)\n",
    "\n",
    "# normalising the data\n",
    "X_train = X/255\n",
    "y_train = y\n",
    "#X_test = X_test/255\n",
    "\n",
    "# converting the train and test data into arrays\n",
    "X_train = np.array(X_train)\n",
    "#X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb45f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b9c192f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "68/68 [==============================] - 12s 167ms/step - loss: 0.7136 - accuracy: 0.5315\n",
      "Epoch 2/40\n",
      "68/68 [==============================] - 11s 166ms/step - loss: 0.6694 - accuracy: 0.5800\n",
      "Epoch 3/40\n",
      "68/68 [==============================] - 11s 167ms/step - loss: 0.6506 - accuracy: 0.6191\n",
      "Epoch 4/40\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.6444 - accuracy: 0.6364\n",
      "Epoch 5/40\n",
      "68/68 [==============================] - 11s 165ms/step - loss: 0.5526 - accuracy: 0.7082\n",
      "Epoch 6/40\n",
      "68/68 [==============================] - 11s 166ms/step - loss: 0.4014 - accuracy: 0.8200\n",
      "Epoch 7/40\n",
      "68/68 [==============================] - 11s 164ms/step - loss: 0.4036 - accuracy: 0.8140\n",
      "Epoch 8/40\n",
      "68/68 [==============================] - 11s 164ms/step - loss: 0.2189 - accuracy: 0.9142\n",
      "Epoch 9/40\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.1435 - accuracy: 0.9520\n",
      "Epoch 10/40\n",
      "68/68 [==============================] - 12s 172ms/step - loss: 0.2280 - accuracy: 0.9184\n",
      "Epoch 11/40\n",
      "68/68 [==============================] - 11s 164ms/step - loss: 0.1026 - accuracy: 0.9753\n",
      "Epoch 12/40\n",
      "68/68 [==============================] - 11s 165ms/step - loss: 0.0878 - accuracy: 0.9814\n",
      "Epoch 13/40\n",
      "68/68 [==============================] - 11s 164ms/step - loss: 0.0842 - accuracy: 0.9832\n",
      "Epoch 14/40\n",
      "68/68 [==============================] - 12s 174ms/step - loss: 0.0721 - accuracy: 0.9841\n",
      "Epoch 15/40\n",
      "68/68 [==============================] - 11s 165ms/step - loss: 0.0955 - accuracy: 0.9758\n",
      "Epoch 16/40\n",
      "68/68 [==============================] - 11s 166ms/step - loss: 0.0731 - accuracy: 0.9851\n",
      "Epoch 17/40\n",
      "68/68 [==============================] - 11s 164ms/step - loss: 0.0747 - accuracy: 0.9809\n",
      "Epoch 18/40\n",
      "68/68 [==============================] - 11s 167ms/step - loss: 0.0661 - accuracy: 0.9865\n",
      "Epoch 19/40\n",
      "68/68 [==============================] - 11s 164ms/step - loss: 0.0691 - accuracy: 0.9837\n",
      "Epoch 20/40\n",
      "68/68 [==============================] - 11s 164ms/step - loss: 0.0652 - accuracy: 0.9879\n",
      "Epoch 21/40\n",
      "68/68 [==============================] - 11s 166ms/step - loss: 0.0656 - accuracy: 0.9860\n",
      "Epoch 22/40\n",
      "68/68 [==============================] - 11s 165ms/step - loss: 0.0539 - accuracy: 0.9879\n",
      "Epoch 23/40\n",
      "68/68 [==============================] - 11s 166ms/step - loss: 0.0556 - accuracy: 0.9879\n",
      "Epoch 24/40\n",
      "68/68 [==============================] - 11s 166ms/step - loss: 0.0513 - accuracy: 0.9869\n",
      "Epoch 25/40\n",
      "68/68 [==============================] - 12s 178ms/step - loss: 0.0583 - accuracy: 0.9841\n",
      "Epoch 26/40\n",
      "68/68 [==============================] - 11s 165ms/step - loss: 0.0558 - accuracy: 0.9860\n",
      "Epoch 27/40\n",
      "68/68 [==============================] - 11s 166ms/step - loss: 0.0503 - accuracy: 0.9869\n",
      "Epoch 28/40\n",
      "68/68 [==============================] - 11s 166ms/step - loss: 0.0516 - accuracy: 0.9879\n",
      "Epoch 29/40\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0575 - accuracy: 0.9846\n",
      "Epoch 30/40\n",
      "68/68 [==============================] - 12s 169ms/step - loss: 0.0555 - accuracy: 0.9883\n",
      "Epoch 31/40\n",
      "68/68 [==============================] - 14s 207ms/step - loss: 0.0595 - accuracy: 0.9865\n",
      "Epoch 32/40\n",
      "68/68 [==============================] - 13s 183ms/step - loss: 0.0625 - accuracy: 0.9860\n",
      "Epoch 33/40\n",
      "68/68 [==============================] - 11s 166ms/step - loss: 0.0709 - accuracy: 0.9841\n",
      "Epoch 34/40\n",
      "68/68 [==============================] - 11s 159ms/step - loss: 0.0680 - accuracy: 0.9846\n",
      "Epoch 35/40\n",
      "68/68 [==============================] - 11s 159ms/step - loss: 0.0477 - accuracy: 0.9883\n",
      "Epoch 36/40\n",
      "68/68 [==============================] - 11s 158ms/step - loss: 0.0536 - accuracy: 0.9869\n",
      "Epoch 37/40\n",
      "68/68 [==============================] - 11s 164ms/step - loss: 0.0502 - accuracy: 0.9874\n",
      "Epoch 38/40\n",
      "68/68 [==============================] - 12s 169ms/step - loss: 0.0550 - accuracy: 0.9846\n",
      "Epoch 39/40\n",
      "68/68 [==============================] - 12s 180ms/step - loss: 0.0530 - accuracy: 0.9888\n",
      "Epoch 40/40\n",
      "68/68 [==============================] - 16s 235ms/step - loss: 0.0506 - accuracy: 0.9860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25e4d0309a0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = models.Sequential([\n",
    "    \n",
    "    layers.Conv2D(filters=32, kernel_size=(2,2), activation='tanh', input_shape=(IMG_SIZE,IMG_SIZE, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=64, kernel_size=(2,2), activation='tanh'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=128, kernel_size=(2,2), activation='tanh'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=128, kernel_size=(2,2), activation='tanh'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(filters=256, kernel_size=(2,2), activation='tanh'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    \n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='tanh'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "#Compiling the model\n",
    "model_3.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#fitting model on training data\n",
    "model_3.fit(np.array(X_train), np.array(y_train), epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89c77132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 48ms/step\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       152\n",
      "           1       0.39      0.28      0.33        25\n",
      "\n",
      "    accuracy                           0.84       177\n",
      "   macro avg       0.64      0.60      0.62       177\n",
      "weighted avg       0.82      0.84      0.82       177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predictions on validation data\n",
    "y_pred2 = model_3.predict(X_test)\n",
    "y_pred2_classes = [np.argmax(element) for element in y_pred2]\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(y_test, y_pred2_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2a21fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.save('model_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db53db8",
   "metadata": {},
   "source": [
    "# Exp - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3dedb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from the drive and saving it to Categories\n",
    "DATADIR =\"C:/Users/AKSHAY LATHWAL/Desktop/Capstone/Final Model/2,3 vs 4\"\n",
    "CATEGORIES = ['2,3','4']\n",
    "\n",
    "# creating the training dataset using the images of all 5 categories\n",
    "\n",
    "training_data = []\n",
    "IMG_SIZE = 150\n",
    "def create_training_data():  # function to create an array of images\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR,category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path,img))\n",
    "            try:\n",
    "                new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE)) # resizing the image\n",
    "            except:\n",
    "                continue\n",
    "            training_data.append([new_array,class_num])\n",
    "\n",
    "create_training_data()\n",
    "\n",
    "# empty lists to append images and labels\n",
    "X = []\n",
    "y = []\n",
    "# appending images to list X and labels to list Y\n",
    "for features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd6a59b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into train and test datasets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)\n",
    "\n",
    "# normalising the data\n",
    "X_train = X/255\n",
    "y_train = y\n",
    "#X_test = X_test/255\n",
    "\n",
    "# converting the train and test data into arrays\n",
    "X_train = np.array(X_train)\n",
    "#X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ecd9db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "68/68 [==============================] - 24s 344ms/step - loss: 0.6938 - accuracy: 0.5114\n",
      "Epoch 2/30\n",
      "68/68 [==============================] - 23s 339ms/step - loss: 0.6943 - accuracy: 0.4895\n",
      "Epoch 3/30\n",
      "68/68 [==============================] - 23s 340ms/step - loss: 0.6897 - accuracy: 0.5431\n",
      "Epoch 4/30\n",
      "68/68 [==============================] - 24s 347ms/step - loss: 0.6806 - accuracy: 0.5688\n",
      "Epoch 5/30\n",
      "68/68 [==============================] - 23s 338ms/step - loss: 0.6790 - accuracy: 0.5711\n",
      "Epoch 6/30\n",
      "68/68 [==============================] - 23s 341ms/step - loss: 0.6596 - accuracy: 0.6168\n",
      "Epoch 7/30\n",
      "68/68 [==============================] - 26s 387ms/step - loss: 0.6518 - accuracy: 0.6117\n",
      "Epoch 8/30\n",
      "68/68 [==============================] - 24s 357ms/step - loss: 0.6418 - accuracy: 0.6298\n",
      "Epoch 9/30\n",
      "68/68 [==============================] - 30s 437ms/step - loss: 0.6134 - accuracy: 0.6657\n",
      "Epoch 10/30\n",
      "68/68 [==============================] - 28s 410ms/step - loss: 0.6094 - accuracy: 0.6755\n",
      "Epoch 11/30\n",
      "68/68 [==============================] - 25s 375ms/step - loss: 0.5392 - accuracy: 0.7133\n",
      "Epoch 12/30\n",
      "68/68 [==============================] - 23s 331ms/step - loss: 0.5655 - accuracy: 0.6844\n",
      "Epoch 13/30\n",
      "68/68 [==============================] - 23s 331ms/step - loss: 0.4701 - accuracy: 0.7664\n",
      "Epoch 14/30\n",
      "68/68 [==============================] - 24s 347ms/step - loss: 0.3663 - accuracy: 0.8275\n",
      "Epoch 15/30\n",
      "68/68 [==============================] - 23s 338ms/step - loss: 0.3322 - accuracy: 0.8615\n",
      "Epoch 16/30\n",
      "68/68 [==============================] - 22s 329ms/step - loss: 0.3843 - accuracy: 0.8378\n",
      "Epoch 17/30\n",
      "68/68 [==============================] - 23s 332ms/step - loss: 0.1953 - accuracy: 0.9347\n",
      "Epoch 18/30\n",
      "68/68 [==============================] - 22s 331ms/step - loss: 0.1655 - accuracy: 0.9455\n",
      "Epoch 19/30\n",
      "68/68 [==============================] - 24s 359ms/step - loss: 0.1376 - accuracy: 0.9543\n",
      "Epoch 20/30\n",
      "68/68 [==============================] - 26s 376ms/step - loss: 0.1117 - accuracy: 0.9646\n",
      "Epoch 21/30\n",
      "68/68 [==============================] - 24s 350ms/step - loss: 0.1481 - accuracy: 0.9497\n",
      "Epoch 22/30\n",
      "68/68 [==============================] - 23s 344ms/step - loss: 0.0860 - accuracy: 0.9753\n",
      "Epoch 23/30\n",
      "68/68 [==============================] - 23s 337ms/step - loss: 0.0670 - accuracy: 0.9837\n",
      "Epoch 24/30\n",
      "68/68 [==============================] - 23s 333ms/step - loss: 0.3047 - accuracy: 0.8690\n",
      "Epoch 25/30\n",
      "68/68 [==============================] - 25s 367ms/step - loss: 0.1845 - accuracy: 0.9343\n",
      "Epoch 26/30\n",
      "68/68 [==============================] - 25s 364ms/step - loss: 0.1057 - accuracy: 0.9706\n",
      "Epoch 27/30\n",
      "68/68 [==============================] - 24s 357ms/step - loss: 0.0721 - accuracy: 0.9855\n",
      "Epoch 28/30\n",
      "68/68 [==============================] - 27s 398ms/step - loss: 0.0526 - accuracy: 0.9874\n",
      "Epoch 29/30\n",
      "68/68 [==============================] - 27s 392ms/step - loss: 0.0508 - accuracy: 0.9860\n",
      "Epoch 30/30\n",
      "68/68 [==============================] - 23s 335ms/step - loss: 0.0692 - accuracy: 0.9828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25e4d101040>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4 = models.Sequential([\n",
    "    \n",
    "    layers.Conv2D(filters=32, kernel_size=(2,2), activation='relu', input_shape=(IMG_SIZE,IMG_SIZE, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=64, kernel_size=(2,2), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=128, kernel_size=(2,2), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=128, kernel_size=(2,2), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(filters=256, kernel_size=(2,2), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    \n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "#Compiling the model\n",
    "model_4.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#fitting model on training data\n",
    "model_4.fit(np.array(X_train), np.array(y_train), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa330e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 98ms/step\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87       152\n",
      "           1       0.23      0.24      0.24        25\n",
      "\n",
      "    accuracy                           0.78       177\n",
      "   macro avg       0.55      0.55      0.55       177\n",
      "weighted avg       0.78      0.78      0.78       177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predictions on validation data\n",
    "y_pred2 = model_4.predict(X_test)\n",
    "y_pred2_classes = [np.argmax(element) for element in y_pred2]\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(y_test, y_pred2_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad674628",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.save('model_4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d8c63",
   "metadata": {},
   "source": [
    "# Exp - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2b0c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from the drive and saving it to Categories\n",
    "DATADIR =\"C:/Users/AKSHAY LATHWAL/Desktop/Capstone/Final Model/2,3 vs 4\"\n",
    "CATEGORIES = ['2,3','4']\n",
    "\n",
    "# creating the training dataset using the images of all 5 categories\n",
    "\n",
    "training_data = []\n",
    "IMG_SIZE = 150\n",
    "def create_training_data():  # function to create an array of images\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR,category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path,img))\n",
    "            try:\n",
    "                new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE)) # resizing the image\n",
    "            except:\n",
    "                continue\n",
    "            training_data.append([new_array,class_num])\n",
    "\n",
    "create_training_data()\n",
    "\n",
    "# empty lists to append images and labels\n",
    "X = []\n",
    "y = []\n",
    "# appending images to list X and labels to list Y\n",
    "for features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a182b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into train and test datasets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)\n",
    "\n",
    "# normalising the data\n",
    "X_train = X/255\n",
    "y_train = y\n",
    "#X_test = X_test/255\n",
    "\n",
    "# converting the train and test data into arrays\n",
    "X_train = np.array(X_train)\n",
    "#X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c397b745",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "68/68 [==============================] - 36s 517ms/step - loss: 0.7054 - accuracy: 0.5557\n",
      "Epoch 2/40\n",
      "68/68 [==============================] - 33s 487ms/step - loss: 0.6916 - accuracy: 0.5543\n",
      "Epoch 3/40\n",
      "68/68 [==============================] - 32s 477ms/step - loss: 0.6473 - accuracy: 0.6224\n",
      "Epoch 4/40\n",
      "68/68 [==============================] - 33s 482ms/step - loss: 0.6744 - accuracy: 0.5818\n",
      "Epoch 5/40\n",
      "68/68 [==============================] - 31s 453ms/step - loss: 0.6408 - accuracy: 0.6494\n",
      "Epoch 6/40\n",
      "68/68 [==============================] - 31s 457ms/step - loss: 0.5878 - accuracy: 0.6867\n",
      "Epoch 7/40\n",
      "68/68 [==============================] - 30s 444ms/step - loss: 0.6507 - accuracy: 0.6452\n",
      "Epoch 8/40\n",
      "68/68 [==============================] - 30s 447ms/step - loss: 0.5083 - accuracy: 0.7506\n",
      "Epoch 9/40\n",
      "68/68 [==============================] - 31s 450ms/step - loss: 0.4199 - accuracy: 0.8033\n",
      "Epoch 10/40\n",
      "68/68 [==============================] - 31s 453ms/step - loss: 0.3267 - accuracy: 0.8564\n",
      "Epoch 11/40\n",
      "68/68 [==============================] - 31s 449ms/step - loss: 0.4765 - accuracy: 0.7902\n",
      "Epoch 12/40\n",
      "68/68 [==============================] - 30s 448ms/step - loss: 0.2252 - accuracy: 0.9329\n",
      "Epoch 13/40\n",
      "68/68 [==============================] - 31s 450ms/step - loss: 0.1722 - accuracy: 0.9520\n",
      "Epoch 14/40\n",
      "68/68 [==============================] - 30s 446ms/step - loss: 0.1418 - accuracy: 0.9678\n",
      "Epoch 15/40\n",
      "68/68 [==============================] - 30s 446ms/step - loss: 0.0888 - accuracy: 0.9828\n",
      "Epoch 16/40\n",
      "68/68 [==============================] - 31s 454ms/step - loss: 0.0867 - accuracy: 0.9818\n",
      "Epoch 17/40\n",
      "68/68 [==============================] - 30s 447ms/step - loss: 0.0762 - accuracy: 0.9855\n",
      "Epoch 18/40\n",
      "68/68 [==============================] - 31s 450ms/step - loss: 0.0660 - accuracy: 0.9869\n",
      "Epoch 19/40\n",
      "68/68 [==============================] - 31s 449ms/step - loss: 0.0847 - accuracy: 0.9832\n",
      "Epoch 20/40\n",
      "68/68 [==============================] - 30s 448ms/step - loss: 0.0709 - accuracy: 0.9846\n",
      "Epoch 21/40\n",
      "68/68 [==============================] - 31s 451ms/step - loss: 0.2748 - accuracy: 0.9007\n",
      "Epoch 22/40\n",
      "68/68 [==============================] - 31s 451ms/step - loss: 0.0968 - accuracy: 0.9786\n",
      "Epoch 23/40\n",
      "68/68 [==============================] - 31s 452ms/step - loss: 0.0957 - accuracy: 0.9776\n",
      "Epoch 24/40\n",
      "68/68 [==============================] - 31s 451ms/step - loss: 0.0720 - accuracy: 0.9860\n",
      "Epoch 25/40\n",
      "68/68 [==============================] - 32s 467ms/step - loss: 0.0650 - accuracy: 0.9860\n",
      "Epoch 26/40\n",
      "68/68 [==============================] - 31s 453ms/step - loss: 0.0590 - accuracy: 0.9869\n",
      "Epoch 27/40\n",
      "68/68 [==============================] - 30s 447ms/step - loss: 0.0624 - accuracy: 0.9874\n",
      "Epoch 28/40\n",
      "68/68 [==============================] - 30s 446ms/step - loss: 0.0520 - accuracy: 0.9879\n",
      "Epoch 29/40\n",
      "68/68 [==============================] - 31s 461ms/step - loss: 0.0720 - accuracy: 0.9855\n",
      "Epoch 30/40\n",
      "68/68 [==============================] - 31s 449ms/step - loss: 0.0526 - accuracy: 0.9888\n",
      "Epoch 31/40\n",
      "68/68 [==============================] - 31s 448ms/step - loss: 0.0493 - accuracy: 0.9888\n",
      "Epoch 32/40\n",
      "68/68 [==============================] - 30s 446ms/step - loss: 0.0494 - accuracy: 0.9897\n",
      "Epoch 33/40\n",
      "68/68 [==============================] - 30s 446ms/step - loss: 0.0428 - accuracy: 0.9883\n",
      "Epoch 34/40\n",
      "68/68 [==============================] - 30s 444ms/step - loss: 0.0471 - accuracy: 0.9879\n",
      "Epoch 35/40\n",
      "68/68 [==============================] - 30s 446ms/step - loss: 0.0395 - accuracy: 0.9907\n",
      "Epoch 36/40\n",
      "68/68 [==============================] - 30s 444ms/step - loss: 0.0378 - accuracy: 0.9911\n",
      "Epoch 37/40\n",
      "68/68 [==============================] - 31s 450ms/step - loss: 0.0347 - accuracy: 0.9902\n",
      "Epoch 38/40\n",
      "68/68 [==============================] - 32s 477ms/step - loss: 0.0317 - accuracy: 0.9911\n",
      "Epoch 39/40\n",
      "68/68 [==============================] - 38s 552ms/step - loss: 0.0336 - accuracy: 0.9888\n",
      "Epoch 40/40\n",
      "68/68 [==============================] - 36s 523ms/step - loss: 0.0324 - accuracy: 0.9911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25e5c3c6bb0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5 = models.Sequential([\n",
    "    \n",
    "    layers.Conv2D(filters=32, kernel_size=(2,2), activation='tanh', input_shape=(IMG_SIZE,IMG_SIZE, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=64, kernel_size=(2,2), activation='tanh'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=128, kernel_size=(2,2), activation='tanh'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=128, kernel_size=(2,2), activation='tanh'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(filters=256, kernel_size=(2,2), activation='tanh'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    \n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='tanh'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "#Compiling the model\n",
    "model_5.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#fitting model on training data\n",
    "model_5.fit(np.array(X_train), np.array(y_train), epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf6d66fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Laoding Val_Test data\n",
    "DATADIR =\"C:/Users/AKSHAY LATHWAL/Desktop/Capstone/Final Model/Val_Test(2,3 vs 4)\"\n",
    "CATEGORIES = ['2,3','4']\n",
    "\n",
    "# creating the val_test dataset using the images of all 5 categories\n",
    "validation_data = []\n",
    "IMG_SIZE = 150\n",
    "def create_validation_data():  # function to create an array of images\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR,category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path,img))\n",
    "            try:\n",
    "                new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE)) # resizing the image\n",
    "            except:\n",
    "                continue\n",
    "            validation_data.append([new_array,class_num])\n",
    "\n",
    "create_validation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51c7c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists to append images and labels\n",
    "X = []\n",
    "y = []\n",
    "# appending images to list X and labels to list Y\n",
    "for features,label in validation_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X)\n",
    "\n",
    "# normalising the data\n",
    "X_test = X/255\n",
    "y_test = y\n",
    "\n",
    "# converting the train and test data into arrays\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef7ca7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 103ms/step\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90       152\n",
      "           1       0.38      0.32      0.35        25\n",
      "\n",
      "    accuracy                           0.83       177\n",
      "   macro avg       0.64      0.62      0.63       177\n",
      "weighted avg       0.82      0.83      0.82       177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predictions on validation data\n",
    "y_pred2 = model_5.predict(X_test)\n",
    "y_pred2_classes = [np.argmax(element) for element in y_pred2]\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(y_test, y_pred2_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "11d8e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5.save('model_5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b2c14f",
   "metadata": {},
   "source": [
    "# Exp - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "555bd687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from the drive and saving it to Categories\n",
    "DATADIR =\"C:/Users/AKSHAY LATHWAL/Desktop/Capstone/Final Model/2,3 vs 4\"\n",
    "CATEGORIES = ['2,3','4']\n",
    "\n",
    "# creating the training dataset using the images of all 5 categories\n",
    "\n",
    "training_data = []\n",
    "IMG_SIZE = 200\n",
    "def create_training_data():  # function to create an array of images\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR,category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path,img))\n",
    "            try:\n",
    "                new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE)) # resizing the image\n",
    "            except:\n",
    "                continue\n",
    "            training_data.append([new_array,class_num])\n",
    "\n",
    "create_training_data()\n",
    "\n",
    "# empty lists to append images and labels\n",
    "X = []\n",
    "y = []\n",
    "# appending images to list X and labels to list Y\n",
    "for features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61e141fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into train and test datasets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)\n",
    "\n",
    "# normalising the data\n",
    "X_train = X/255\n",
    "y_train = y\n",
    "#X_test = X_test/255\n",
    "\n",
    "# converting the train and test data into arrays\n",
    "X_train = np.array(X_train)\n",
    "#X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ea7414e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "68/68 [==============================] - 57s 741ms/step - loss: 0.6942 - accuracy: 0.5044\n",
      "Epoch 2/30\n",
      "68/68 [==============================] - 42s 616ms/step - loss: 0.6942 - accuracy: 0.5086\n",
      "Epoch 3/30\n",
      "68/68 [==============================] - 41s 600ms/step - loss: 0.6934 - accuracy: 0.5054\n",
      "Epoch 4/30\n",
      "68/68 [==============================] - 41s 596ms/step - loss: 0.6932 - accuracy: 0.5035\n",
      "Epoch 5/30\n",
      "68/68 [==============================] - 41s 604ms/step - loss: 0.6932 - accuracy: 0.5035\n",
      "Epoch 6/30\n",
      "68/68 [==============================] - 41s 610ms/step - loss: 0.6931 - accuracy: 0.5035\n",
      "Epoch 7/30\n",
      "68/68 [==============================] - 42s 622ms/step - loss: 0.6932 - accuracy: 0.5002\n",
      "Epoch 8/30\n",
      "68/68 [==============================] - 41s 598ms/step - loss: 0.6952 - accuracy: 0.5054\n",
      "Epoch 9/30\n",
      "68/68 [==============================] - 40s 595ms/step - loss: 0.6932 - accuracy: 0.5035\n",
      "Epoch 10/30\n",
      "68/68 [==============================] - 44s 650ms/step - loss: 0.6932 - accuracy: 0.4993\n",
      "Epoch 11/30\n",
      "68/68 [==============================] - 41s 597ms/step - loss: 0.6932 - accuracy: 0.5035\n",
      "Epoch 12/30\n",
      "68/68 [==============================] - 41s 609ms/step - loss: 0.6932 - accuracy: 0.4914\n",
      "Epoch 13/30\n",
      "68/68 [==============================] - 41s 597ms/step - loss: 0.6932 - accuracy: 0.5035\n",
      "Epoch 14/30\n",
      "68/68 [==============================] - 40s 594ms/step - loss: 0.6933 - accuracy: 0.5035\n",
      "Epoch 15/30\n",
      "68/68 [==============================] - 40s 590ms/step - loss: 0.6932 - accuracy: 0.5035\n",
      "Epoch 16/30\n",
      "68/68 [==============================] - 40s 592ms/step - loss: 0.6932 - accuracy: 0.5035\n",
      "Epoch 17/30\n",
      "68/68 [==============================] - 41s 598ms/step - loss: 0.6932 - accuracy: 0.5035\n",
      "Epoch 18/30\n",
      "68/68 [==============================] - 40s 591ms/step - loss: 0.6932 - accuracy: 0.4839\n",
      "Epoch 19/30\n",
      "68/68 [==============================] - 42s 613ms/step - loss: 0.6932 - accuracy: 0.5035\n",
      "Epoch 20/30\n",
      "68/68 [==============================] - 40s 593ms/step - loss: 0.6932 - accuracy: 0.4783\n",
      "Epoch 21/30\n",
      "68/68 [==============================] - 41s 609ms/step - loss: 0.6932 - accuracy: 0.4862\n",
      "Epoch 22/30\n",
      "68/68 [==============================] - 40s 589ms/step - loss: 0.6932 - accuracy: 0.5035\n",
      "Epoch 23/30\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.6933 - accuracy: 0.5035\n",
      "Epoch 24/30\n",
      "68/68 [==============================] - 41s 610ms/step - loss: 0.6932 - accuracy: 0.5035\n",
      "Epoch 25/30\n",
      "68/68 [==============================] - 40s 589ms/step - loss: 0.6932 - accuracy: 0.5035\n",
      "Epoch 26/30\n",
      "68/68 [==============================] - 40s 590ms/step - loss: 0.6932 - accuracy: 0.4867\n",
      "Epoch 27/30\n",
      "68/68 [==============================] - 40s 594ms/step - loss: 0.6933 - accuracy: 0.4830\n",
      "Epoch 28/30\n",
      "68/68 [==============================] - 40s 592ms/step - loss: 0.6932 - accuracy: 0.5035\n",
      "Epoch 29/30\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.6932 - accuracy: 0.4821\n",
      "Epoch 30/30\n",
      "68/68 [==============================] - 40s 595ms/step - loss: 0.6932 - accuracy: 0.5035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25e567428e0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6 = models.Sequential([\n",
    "    \n",
    "    layers.Conv2D(filters=32, kernel_size=(2,2), activation='relu', input_shape=(IMG_SIZE,IMG_SIZE, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=64, kernel_size=(2,2), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=128, kernel_size=(2,2), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=128, kernel_size=(2,2), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(filters=256, kernel_size=(2,2), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    \n",
    "\n",
    "    layers.Conv2D(filters=256, kernel_size=(2,2), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    \n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "#Compiling the model\n",
    "model_6.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#fitting model on training data\n",
    "model_6.fit(np.array(X_train), np.array(y_train), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "11affd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Laoding Val_Test data\n",
    "DATADIR =\"C:/Users/AKSHAY LATHWAL/Desktop/Capstone/Final Model/Val_Test(2,3 vs 4)\"\n",
    "CATEGORIES = ['2,3','4']\n",
    "\n",
    "# creating the val_test dataset using the images of all 5 categories\n",
    "validation_data = []\n",
    "IMG_SIZE = 200\n",
    "def create_validation_data():  # function to create an array of images\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR,category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path,img))\n",
    "            try:\n",
    "                new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE)) # resizing the image\n",
    "            except:\n",
    "                continue\n",
    "            validation_data.append([new_array,class_num])\n",
    "\n",
    "create_validation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "550a2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists to append images and labels\n",
    "X = []\n",
    "y = []\n",
    "# appending images to list X and labels to list Y\n",
    "for features,label in validation_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X)\n",
    "\n",
    "# normalising the data\n",
    "X_test = X/255\n",
    "y_test = y\n",
    "\n",
    "# converting the train and test data into arrays\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8b123c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 135ms/step\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       152\n",
      "           1       0.14      1.00      0.25        25\n",
      "\n",
      "    accuracy                           0.14       177\n",
      "   macro avg       0.07      0.50      0.12       177\n",
      "weighted avg       0.02      0.14      0.03       177\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AKSHAY LATHWAL\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\AKSHAY LATHWAL\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\AKSHAY LATHWAL\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# predictions on validation data\n",
    "y_pred2 = model_6.predict(X_test)\n",
    "y_pred2_classes = [np.argmax(element) for element in y_pred2]\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(y_test, y_pred2_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9dc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6.save('model_6.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a9109",
   "metadata": {},
   "source": [
    "# Exp - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "37a88db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from the drive and saving it to Categories\n",
    "DATADIR =\"C:/Users/AKSHAY LATHWAL/Desktop/Capstone/Final Model/2,3 vs 4\"\n",
    "CATEGORIES = ['2,3','4']\n",
    "\n",
    "# creating the training dataset using the images of all 5 categories\n",
    "\n",
    "training_data = []\n",
    "IMG_SIZE = 200\n",
    "def create_training_data():  # function to create an array of images\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR,category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path,img))\n",
    "            try:\n",
    "                new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE)) # resizing the image\n",
    "            except:\n",
    "                continue\n",
    "            training_data.append([new_array,class_num])\n",
    "\n",
    "create_training_data()\n",
    "\n",
    "# empty lists to append images and labels\n",
    "X = []\n",
    "y = []\n",
    "# appending images to list X and labels to list Y\n",
    "for features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "764c0388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into train and test datasets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)\n",
    "\n",
    "# normalising the data\n",
    "X_train = X/255\n",
    "y_train = y\n",
    "#X_test = X_test/255\n",
    "\n",
    "# converting the train and test data into arrays\n",
    "X_train = np.array(X_train)\n",
    "#X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f64baabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "68/68 [==============================] - 49s 687ms/step - loss: 0.7882 - accuracy: 0.5212\n",
      "Epoch 2/30\n",
      "68/68 [==============================] - 50s 739ms/step - loss: 0.6796 - accuracy: 0.5758\n",
      "Epoch 3/30\n",
      "68/68 [==============================] - 46s 676ms/step - loss: 0.6635 - accuracy: 0.5855\n",
      "Epoch 4/30\n",
      "68/68 [==============================] - 46s 677ms/step - loss: 0.6410 - accuracy: 0.6308\n",
      "Epoch 5/30\n",
      "68/68 [==============================] - 45s 667ms/step - loss: 0.6024 - accuracy: 0.6807\n",
      "Epoch 6/30\n",
      "68/68 [==============================] - 54s 801ms/step - loss: 0.5233 - accuracy: 0.7399\n",
      "Epoch 7/30\n",
      "68/68 [==============================] - 51s 752ms/step - loss: 0.4585 - accuracy: 0.7837\n",
      "Epoch 8/30\n",
      "68/68 [==============================] - 50s 732ms/step - loss: 0.3410 - accuracy: 0.8625\n",
      "Epoch 9/30\n",
      "68/68 [==============================] - 44s 652ms/step - loss: 0.2108 - accuracy: 0.9175\n",
      "Epoch 10/30\n",
      "68/68 [==============================] - 45s 655ms/step - loss: 0.1622 - accuracy: 0.9562\n",
      "Epoch 11/30\n",
      "68/68 [==============================] - 44s 649ms/step - loss: 0.1121 - accuracy: 0.9716\n",
      "Epoch 12/30\n",
      "68/68 [==============================] - 45s 662ms/step - loss: 0.1249 - accuracy: 0.9730\n",
      "Epoch 13/30\n",
      "68/68 [==============================] - 45s 657ms/step - loss: 0.0931 - accuracy: 0.9828\n",
      "Epoch 14/30\n",
      "68/68 [==============================] - 45s 661ms/step - loss: 0.0883 - accuracy: 0.9814\n",
      "Epoch 15/30\n",
      "68/68 [==============================] - 45s 665ms/step - loss: 0.0720 - accuracy: 0.9837\n",
      "Epoch 16/30\n",
      "68/68 [==============================] - 45s 656ms/step - loss: 0.0683 - accuracy: 0.9865\n",
      "Epoch 17/30\n",
      "68/68 [==============================] - 44s 653ms/step - loss: 0.0585 - accuracy: 0.9869\n",
      "Epoch 18/30\n",
      "68/68 [==============================] - 44s 652ms/step - loss: 0.0590 - accuracy: 0.9860\n",
      "Epoch 19/30\n",
      "68/68 [==============================] - 45s 657ms/step - loss: 0.0637 - accuracy: 0.9869\n",
      "Epoch 20/30\n",
      "68/68 [==============================] - 46s 670ms/step - loss: 0.0513 - accuracy: 0.9874\n",
      "Epoch 21/30\n",
      "68/68 [==============================] - 45s 656ms/step - loss: 0.0422 - accuracy: 0.9883\n",
      "Epoch 22/30\n",
      "68/68 [==============================] - 45s 656ms/step - loss: 0.0355 - accuracy: 0.9874\n",
      "Epoch 23/30\n",
      "68/68 [==============================] - 45s 656ms/step - loss: 0.0379 - accuracy: 0.9879\n",
      "Epoch 24/30\n",
      "68/68 [==============================] - 45s 658ms/step - loss: 0.0322 - accuracy: 0.9907\n",
      "Epoch 25/30\n",
      "68/68 [==============================] - 46s 673ms/step - loss: 0.0349 - accuracy: 0.9893\n",
      "Epoch 26/30\n",
      "68/68 [==============================] - 45s 668ms/step - loss: 0.0285 - accuracy: 0.9897\n",
      "Epoch 27/30\n",
      "68/68 [==============================] - 44s 654ms/step - loss: 0.0269 - accuracy: 0.9902\n",
      "Epoch 28/30\n",
      "68/68 [==============================] - 45s 659ms/step - loss: 0.0268 - accuracy: 0.9916\n",
      "Epoch 29/30\n",
      "68/68 [==============================] - 45s 664ms/step - loss: 0.0253 - accuracy: 0.9921\n",
      "Epoch 30/30\n",
      "68/68 [==============================] - 45s 661ms/step - loss: 0.0233 - accuracy: 0.9921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25e488392e0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_7 = models.Sequential([\n",
    "    \n",
    "    layers.Conv2D(filters=32, kernel_size=(2,2), activation='tanh', input_shape=(IMG_SIZE,IMG_SIZE, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=64, kernel_size=(2,2), activation='tanh'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=128, kernel_size=(2,2), activation='tanh'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=128, kernel_size=(2,2), activation='tanh'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(filters=256, kernel_size=(2,2), activation='tanh'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "\n",
    "    layers.Conv2D(filters=256, kernel_size=(2,2), activation='tanh'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='tanh'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "#Compiling the model\n",
    "model_7.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#fitting model on training data\n",
    "model_7.fit(np.array(X_train), np.array(y_train), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "02ddc87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Laoding Val_Test data\n",
    "DATADIR =\"C:/Users/AKSHAY LATHWAL/Desktop/Capstone/Final Model/Val_Test(2,3 vs 4)\"\n",
    "CATEGORIES = ['2,3','4']\n",
    "\n",
    "# creating the val_test dataset using the images of all 5 categories\n",
    "validation_data = []\n",
    "IMG_SIZE = 200\n",
    "def create_validation_data():  # function to create an array of images\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR,category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path,img))\n",
    "            try:\n",
    "                new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE)) # resizing the image\n",
    "            except:\n",
    "                continue\n",
    "            validation_data.append([new_array,class_num])\n",
    "\n",
    "create_validation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ffe9b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists to append images and labels\n",
    "X = []\n",
    "y = []\n",
    "# appending images to list X and labels to list Y\n",
    "for features,label in validation_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X)\n",
    "\n",
    "# normalising the data\n",
    "X_test = X/255\n",
    "y_test = y\n",
    "\n",
    "# converting the train and test data into arrays\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "599c44fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 152ms/step\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91       152\n",
      "           1       0.36      0.16      0.22        25\n",
      "\n",
      "    accuracy                           0.84       177\n",
      "   macro avg       0.62      0.56      0.57       177\n",
      "weighted avg       0.80      0.84      0.81       177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predictions on validation data\n",
    "y_pred2 = model_7.predict(X_test)\n",
    "y_pred2_classes = [np.argmax(element) for element in y_pred2]\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(y_test, y_pred2_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "62f956fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7.save('model_7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb523e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
